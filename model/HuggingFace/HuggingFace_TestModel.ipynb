{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ac23d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e2031",
   "metadata": {},
   "source": [
    "## Using the model we trained (2 Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5b3ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_0\n"
     ]
    }
   ],
   "source": [
    "#test with autotokenizer\n",
    "newTokenizer = AutoTokenizer.from_pretrained(\"gohbwj/sentiment-fine-tuned-yelp-2L\")\n",
    "newModel = AutoModelForSequenceClassification.from_pretrained(\"gohbwj/sentiment-fine-tuned-yelp-2L\")\n",
    "\n",
    "text = \"Order here all year. Yesterday after making me wait for 3 hours! Food still wasn't ready. From 45 minutes delivery to more than 3 hours! Guess people don't appreciate long term customers.\"\n",
    "encoded_input = newTokenizer(text, return_tensors='pt')\n",
    "output = newModel(**encoded_input)\n",
    "print(newModel.config.id2label[output.logits.argmax().item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1084d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e09d42b7c3a41e1b92d99786a3e969d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/982 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.9973719120025635}],\n",
       " [{'label': 'LABEL_1', 'score': 0.9998078942298889}],\n",
       " [{'label': 'LABEL_1', 'score': 0.9978553652763367}],\n",
       " [{'label': 'LABEL_0', 'score': 0.9983545541763306}],\n",
       " [{'label': 'LABEL_0', 'score': 0.504268229007721}]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test with pipeline\n",
    "sentiment_model = pipeline(model=\"gohbwj/sentiment-fine-tuned-yelp-2L\")\n",
    "\n",
    "sentiment_model([\"Order here all year. Yesterday after making me wait for 3 hours! Food still wasn't ready. From 45 minutes delivery to more than 3 hours! Guess people don't appreciate long term customers.\",\n",
    "                 \"They have an excellent selection even though it is in a Coffeeshop, in fact they taste better than some of the actual restaurant I went to.\",\n",
    "                \"The waffles were really not bad. The idea of the beaker for the syrup is creative and the yuzu ice cream wasn‚Äôt icy or too sugary.\",\n",
    "                \"Staff let customers wait outside for too long time. There were just 3 people in front of me, and there were really enough sits in the restaurant. Howerver, I waited for 30 minutes just for takeaway. Staff didn't care the people waiting and moved so slowly. The worst mcdonals I've ever visited in Singapore.\",\n",
    "                \"Small family diner with cozy vibes but the quality of food was somewhat subpar.\"],\n",
    "               top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315cf20",
   "metadata": {},
   "source": [
    "### Test on labelled_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5174e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-bbf04892c4815010\n",
      "Found cached dataset csv (C:/Users/Admin/.cache/huggingface/datasets/csv/default-bbf04892c4815010/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95412e998ed442fb8eb39e0103460bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load\n",
    "test_dataset = load_dataset('csv', data_files=\"..\\\\..\\\\scraper\\\\data\\\\labelled_10k.csv\")\n",
    "\n",
    "def tokenizeFunction(examples):\n",
    "    return newTokenizer(examples[\"text\"], max_length=550, padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d705361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hold a copy as df for later\n",
    "backupdf = pd.DataFrame(test_dataset['train'])\n",
    "\n",
    "#get only the input data\n",
    "test_dataset = test_dataset.remove_columns('restaurant name')\n",
    "test_dataset = test_dataset.remove_columns('rating')\n",
    "test_dataset = test_dataset.remove_columns('Tokenized')\n",
    "test_dataset = test_dataset.remove_columns('predicted_subjectivity')\n",
    "test_dataset = test_dataset.remove_columns('label')\n",
    "test_dataset = test_dataset.remove_columns('content')\n",
    "test_dataset = test_dataset.rename_column('content_clean', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bc5bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874c875692a240f69dc47527e797e395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#need to trunctate as model only handles max 512 tensors\n",
    "def trimNtrunc(examples):\n",
    "    try:\n",
    "        examples['text'] = examples['text'].replace(r'\\n', '')\n",
    "        if len(examples['text']) > 512:\n",
    "            examples['text'] = examples['text'][:512]\n",
    "    except:\n",
    "        examples['text'] = ''\n",
    "        pass\n",
    "    return examples\n",
    "\n",
    "test_dataset = test_dataset['train'].map(trimNtrunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83dadeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d560ccca454d49892acdf3fd708de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize the datasets\n",
    "tokenized_dataset = test_dataset.map(tokenizeFunction, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30cbcc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 21s\n",
      "Wall time: 32.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#run thru the model and get output\n",
    "output = sentiment_model(tokenized_dataset['text'], top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70ec8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some magic to format the data\n",
    "outputdf = pd.DataFrame(list(chain.from_iterable(output)))\n",
    "evendf = outputdf.iloc[0:len(outputdf):2]\n",
    "odddf = outputdf.iloc[1:len(outputdf):2]\n",
    "evendf.reset_index(drop=True, inplace=True)\n",
    "odddf.reset_index(drop=True, inplace=True)\n",
    "outputdf = evendf.join(odddf, lsuffix=\"_pred\", rsuffix=\"_other\")\n",
    "\n",
    "outputdf['label_pred'] = outputdf['label_pred'].replace({'LABEL_0':0.0, 'LABEL_1':1.0})\n",
    "outputdf['label_other'] = outputdf['label_other'].replace({'LABEL_0':0.0, 'LABEL_1':1.0})\n",
    "\n",
    "# def pred_label(row):\n",
    "#     if row['score_0'] > row['score_1']:\n",
    "#         return 0.0\n",
    "#     if row['score_1'] > row['score_0']:\n",
    "#         return 1.0\n",
    "#     return 1.0\n",
    "\n",
    "# def pred_score(row):\n",
    "#     if row['label_pred'] == 0.0:\n",
    "#         return row['score_0']\n",
    "#     if row['label_pred'] == 1.0:\n",
    "#         return row['score_1']\n",
    "\n",
    "# outputdf['label_pred'] = outputdf.apply (lambda row: pred_label(row), axis=1)\n",
    "# outputdf['score_pred'] = outputdf.apply (lambda row: pred_score(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5339c47",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant name</th>\n",
       "      <th>rating</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>predicted_subjectivity</th>\n",
       "      <th>label_pred</th>\n",
       "      <th>score_pred</th>\n",
       "      <th>label_other</th>\n",
       "      <th>score_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Konomi Zen</td>\n",
       "      <td>3</td>\n",
       "      <td>Crunchy tempura esp the vegetables</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Crunchy the vegetable</td>\n",
       "      <td>[[('Crunchy', 'NNP'), ('the', 'DT'), ('vegetab...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vincent Western Food</td>\n",
       "      <td>5</td>\n",
       "      <td>this is one of the best western food i've eate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this is one of the best western food eaten the...</td>\n",
       "      <td>[[('this', 'DT'), ('is', 'VBZ'), ('one', 'CD')...</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siam Square Mookata - Best Mookata Restaurant ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Many choice of food to select. Love their teri...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Many choice of food to select Love their pork ...</td>\n",
       "      <td>[[('Many', 'JJ'), ('choice', 'NN'), ('of', 'IN...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.766087</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.233913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Old Chang Kee</td>\n",
       "      <td>1</td>\n",
       "      <td>Buying snacks for customers but system mainten...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>snack for customer but system maintenance cant...</td>\n",
       "      <td>[[('snack', 'NN'), ('for', 'IN'), ('customer',...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.988703</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.011297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiang Ji Cantonese Roasts</td>\n",
       "      <td>1</td>\n",
       "      <td>Seriously overprice and rude service. Avoid at...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Seriously overprice and rude service Avoid at ...</td>\n",
       "      <td>[[('Seriously', 'RB'), ('overprice', 'NN'), ('...</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999337</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Prata Raya</td>\n",
       "      <td>3</td>\n",
       "      <td>Mutton Nasi Biryani\\n\\nüçú Food wise: Overall it...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Mutton Nasi  ramen Food wise Overall it wa a d...</td>\n",
       "      <td>[[('Mutton', 'NNP'), ('Nasi', 'NNP'), ('ramen'...</td>\n",
       "      <td>0.584524</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Chui Xiang Kitchen</td>\n",
       "      <td>4</td>\n",
       "      <td>Really nice Zi Char place and pretty affordabl...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Really nice Char place and pretty affordable t...</td>\n",
       "      <td>[[('Really', 'RB'), ('nice', 'JJ'), ('Char', '...</td>\n",
       "      <td>0.710000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999884</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>A Hot Hideout</td>\n",
       "      <td>5</td>\n",
       "      <td>honestly the best mala i‚Äôve had in singapore, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>honestly the best mala i  ‚Äô had in and the way...</td>\n",
       "      <td>[[('honestly', 'RB'), ('the', 'DT'), ('best', ...</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Chui Huay Lim Teochew Cuisine ÈÜâËä±ÊûóÂìÅÊΩÆËΩ©</td>\n",
       "      <td>3</td>\n",
       "      <td>Difficult to get to, food quality is good, the...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Difficult to get to food quality is good the p...</td>\n",
       "      <td>[[('Difficult', 'NN'), ('to', 'TO'), ('get', '...</td>\n",
       "      <td>0.545212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.992883</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Grains &amp; Hops Food Court Bistro</td>\n",
       "      <td>4</td>\n",
       "      <td>Some good food. Mainly good for the cheap and ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Some good food Mainly good for the cheap and w...</td>\n",
       "      <td>[[('Some', 'DT'), ('good', 'JJ'), ('food', 'NN...</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.778888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.221112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       restaurant name  rating  \\\n",
       "0                                           Konomi Zen       3   \n",
       "1                                 Vincent Western Food       5   \n",
       "2    Siam Square Mookata - Best Mookata Restaurant ...       5   \n",
       "3                                        Old Chang Kee       1   \n",
       "4                            Hiang Ji Cantonese Roasts       1   \n",
       "..                                                 ...     ...   \n",
       "995                                         Prata Raya       3   \n",
       "996                                 Chui Xiang Kitchen       4   \n",
       "997                                      A Hot Hideout       5   \n",
       "998               Chui Huay Lim Teochew Cuisine ÈÜâËä±ÊûóÂìÅÊΩÆËΩ©       3   \n",
       "999                    Grains & Hops Food Court Bistro       4   \n",
       "\n",
       "                                               content  label  \\\n",
       "0                   Crunchy tempura esp the vegetables    2.0   \n",
       "1    this is one of the best western food i've eate...    1.0   \n",
       "2    Many choice of food to select. Love their teri...    1.0   \n",
       "3    Buying snacks for customers but system mainten...    0.0   \n",
       "4    Seriously overprice and rude service. Avoid at...    0.0   \n",
       "..                                                 ...    ...   \n",
       "995  Mutton Nasi Biryani\\n\\nüçú Food wise: Overall it...    1.0   \n",
       "996  Really nice Zi Char place and pretty affordabl...    1.0   \n",
       "997  honestly the best mala i‚Äôve had in singapore, ...    1.0   \n",
       "998  Difficult to get to, food quality is good, the...    1.0   \n",
       "999  Some good food. Mainly good for the cheap and ...    1.0   \n",
       "\n",
       "                                         content_clean  \\\n",
       "0                                Crunchy the vegetable   \n",
       "1    this is one of the best western food eaten the...   \n",
       "2    Many choice of food to select Love their pork ...   \n",
       "3    snack for customer but system maintenance cant...   \n",
       "4    Seriously overprice and rude service Avoid at ...   \n",
       "..                                                 ...   \n",
       "995  Mutton Nasi  ramen Food wise Overall it wa a d...   \n",
       "996  Really nice Char place and pretty affordable t...   \n",
       "997  honestly the best mala i  ‚Äô had in and the way...   \n",
       "998  Difficult to get to food quality is good the p...   \n",
       "999  Some good food Mainly good for the cheap and w...   \n",
       "\n",
       "                                             Tokenized  \\\n",
       "0    [[('Crunchy', 'NNP'), ('the', 'DT'), ('vegetab...   \n",
       "1    [[('this', 'DT'), ('is', 'VBZ'), ('one', 'CD')...   \n",
       "2    [[('Many', 'JJ'), ('choice', 'NN'), ('of', 'IN...   \n",
       "3    [[('snack', 'NN'), ('for', 'IN'), ('customer',...   \n",
       "4    [[('Seriously', 'RB'), ('overprice', 'NN'), ('...   \n",
       "..                                                 ...   \n",
       "995  [[('Mutton', 'NNP'), ('Nasi', 'NNP'), ('ramen'...   \n",
       "996  [[('Really', 'RB'), ('nice', 'JJ'), ('Char', '...   \n",
       "997  [[('honestly', 'RB'), ('the', 'DT'), ('best', ...   \n",
       "998  [[('Difficult', 'NN'), ('to', 'TO'), ('get', '...   \n",
       "999  [[('Some', 'DT'), ('good', 'JJ'), ('food', 'NN...   \n",
       "\n",
       "     predicted_subjectivity  label_pred  score_pred  label_other  score_other  \n",
       "0                  0.000000         1.0    0.999585          0.0     0.000415  \n",
       "1                  0.307692         1.0    0.999727          0.0     0.000273  \n",
       "2                  0.633333         0.0    0.766087          1.0     0.233913  \n",
       "3                  0.000000         0.0    0.988703          1.0     0.011297  \n",
       "4                  0.633333         0.0    0.999337          1.0     0.000662  \n",
       "..                      ...         ...         ...          ...          ...  \n",
       "995                0.584524         1.0    0.994324          0.0     0.005676  \n",
       "996                0.710000         1.0    0.999884          0.0     0.000116  \n",
       "997                0.637500         1.0    0.999886          0.0     0.000114  \n",
       "998                0.545212         0.0    0.992883          1.0     0.007117  \n",
       "999                0.516667         1.0    0.778888          0.0     0.221112  \n",
       "\n",
       "[1000 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine back\n",
    "combine_result = pd.concat([backupdf,  outputdf], axis=1)\n",
    "combine_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "243c51be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.879\n",
      "Precision: 0.830\n",
      "Recall: 0.879\n",
      "F1: 0.853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Desktop\\NLP temp\\Model\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# get metric scores\n",
    "print('Accuracy: %.3f' % accuracy_score(combine_result['label'], combine_result['label_pred']))\n",
    "print('Precision: %.3f' % precision_score(combine_result['label'], combine_result['label_pred'], average=\"weighted\"))\n",
    "print('Recall: %.3f' % recall_score(combine_result['label'], combine_result['label_pred'], average=\"weighted\"))\n",
    "print('F1: %.3f' % f1_score(combine_result['label'], combine_result['label_pred'], average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8eaa945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_19784\\2559852129.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  single_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\", \"score_pred\": \"score\"}, inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_19784\\2559852129.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  double_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#save to csv\n",
    "#version 1 (text, label, score)\n",
    "single_score_version = combine_result[['content_clean', 'label_pred', 'score_pred']]\n",
    "single_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\", \"score_pred\": \"score\"}, inplace=True)\n",
    "single_score_version.to_csv(\"DistilBert_pred_labelled10k_single.csv\", index=False)\n",
    "\n",
    "#version 2 (text, label, score_pred, score_other)\n",
    "double_score_version = combine_result[['content_clean', 'label_pred', 'score_pred', 'score_other']]\n",
    "double_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\"}, inplace=True)\n",
    "double_score_version.to_csv(\"DistilBert_pred_labelled10k_double.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7fea3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (1000, 7)\n",
      "single_score_version: (1000, 3)\n",
      "double_score_version: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "#just make sure the shapes same as original set\n",
    "print(\"original: \" + str(backupdf.shape))\n",
    "print(\"single_score_version: \" + str(single_score_version.shape))\n",
    "print(\"double_score_version: \" + str(double_score_version.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8136fa",
   "metadata": {},
   "source": [
    "### Test on unlabelled_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b67ba921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-d1918eb279f8da62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/Admin/.cache/huggingface/datasets/csv/default-d1918eb279f8da62/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3444d9fb0b29428b9086e44c8e29eeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa247d34de04445fb0db9186623730e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/Admin/.cache/huggingface/datasets/csv/default-d1918eb279f8da62/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Desktop\\NLP temp\\Model\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:697: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8a130cbea1466eb547603fde2748ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load\n",
    "test_dataset = load_dataset('csv', data_files=\"..\\\\..\\\\scraper\\\\data\\\\unlabelled_10k.csv\")\n",
    "\n",
    "def tokenizeFunction(examples):\n",
    "    return newTokenizer(examples[\"text\"], max_length=550, padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47778d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hold a copy as df for later\n",
    "backupdf = pd.DataFrame(test_dataset['train'])\n",
    "\n",
    "#get only the input data\n",
    "test_dataset = test_dataset.remove_columns('restaurant name')\n",
    "test_dataset = test_dataset.remove_columns('rating')\n",
    "test_dataset = test_dataset.remove_columns('Tokenized')\n",
    "test_dataset = test_dataset.remove_columns('predicted_subjectivity')\n",
    "test_dataset = test_dataset.remove_columns('label')\n",
    "test_dataset = test_dataset.remove_columns('content')\n",
    "test_dataset = test_dataset.rename_column('content_clean', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de1fb3bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cedef5be6bb4c208496dad6ab13e60b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8895 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#need to trunctate as model only handles max 512 tensors\n",
    "def trimNtrunc(examples):\n",
    "    try:\n",
    "        examples['text'] = examples['text'].replace(r'\\n', '')\n",
    "        if len(examples['text']) > 512:\n",
    "            examples['text'] = examples['text'][:512]\n",
    "    except:\n",
    "        examples['text'] = ''\n",
    "        pass\n",
    "    return examples\n",
    "test_dataset = test_dataset['train'].map(trimNtrunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7d208ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e3c4311f82473cb72efc71013cd911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize the datasets\n",
    "tokenized_dataset = test_dataset.map(tokenizeFunction, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b821dfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#run thru the model and get output\n",
    "output = sentiment_model(tokenized_dataset['text'], top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53ab8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some magic to format the data\n",
    "outputdf = pd.DataFrame(list(chain.from_iterable(output)))\n",
    "evendf = outputdf.iloc[0:len(outputdf):2]\n",
    "odddf = outputdf.iloc[1:len(outputdf):2]\n",
    "evendf.reset_index(drop=True, inplace=True)\n",
    "odddf.reset_index(drop=True, inplace=True)\n",
    "outputdf = evendf.join(odddf, lsuffix=\"_pred\", rsuffix=\"_other\")\n",
    "\n",
    "outputdf['label_pred'] = outputdf['label_pred'].replace({'LABEL_0':0.0, 'LABEL_1':1.0})\n",
    "outputdf['label_other'] = outputdf['label_other'].replace({'LABEL_0':0.0, 'LABEL_1':1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "283a7d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant name</th>\n",
       "      <th>rating</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>predicted_subjectivity</th>\n",
       "      <th>label_pred</th>\n",
       "      <th>score_pred</th>\n",
       "      <th>label_other</th>\n",
       "      <th>score_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TungLok Teahouse</td>\n",
       "      <td>5</td>\n",
       "      <td>Lisa is a very good host and made us feel very...</td>\n",
       "      <td>1</td>\n",
       "      <td>is a very good host and made u feel very welco...</td>\n",
       "      <td>[[('is', 'VBZ'), ('a', 'DT'), ('very', 'RB'), ...</td>\n",
       "      <td>0.468571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaysia Boleh</td>\n",
       "      <td>5</td>\n",
       "      <td>Very nice piping hot claypot with dark sauce a...</td>\n",
       "      <td>1</td>\n",
       "      <td>Very nice piping hot with dark sauce and sesam...</td>\n",
       "      <td>[[('Very', 'RB'), ('nice', 'JJ'), ('piping', '...</td>\n",
       "      <td>0.725714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999867</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Douraku Sushi</td>\n",
       "      <td>5</td>\n",
       "      <td>Such luxuriously enjoyable Omakase ü§ó\\r\\nRich s...</td>\n",
       "      <td>1</td>\n",
       "      <td>Such luxuriously enjoyable hugging face Rich s...</td>\n",
       "      <td>[[('Such', 'JJ'), ('luxuriously', 'RB'), ('enj...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>511 Indian Store</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice India ingredients foods</td>\n",
       "      <td>1</td>\n",
       "      <td>Nice ingredient food</td>\n",
       "      <td>[[('Nice', 'NNP'), ('ingredient', 'NN'), ('foo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.996541</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Teochew Kitchenette</td>\n",
       "      <td>4</td>\n",
       "      <td>Ordered the Stir-Fry Kang Kong and Marmite Chi...</td>\n",
       "      <td>1</td>\n",
       "      <td>Ordered the Stir-Fry Kang and Marmite Chicken ...</td>\n",
       "      <td>[[('Ordered', 'VBN'), ('the', 'DT'), ('Stir-Fr...</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997530</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8890</th>\n",
       "      <td>Ichikokudo Hokkaido Ramen</td>\n",
       "      <td>4</td>\n",
       "      <td>Wasn‚Äôt crowded during the dinner period.\\r\\nDe...</td>\n",
       "      <td>1</td>\n",
       "      <td>‚Äô t crowded during the dinner period . Decent ...</td>\n",
       "      <td>[[('‚Äô', 'JJ'), ('t', 'NN'), ('crowded', 'VBD')...</td>\n",
       "      <td>0.697778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.936226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8891</th>\n",
       "      <td>Kedai Makan Muhajirin</td>\n",
       "      <td>4</td>\n",
       "      <td>Had the mee rebus, mee siam and nasi lemak wit...</td>\n",
       "      <td>1</td>\n",
       "      <td>Had the rebus , and nasi with , all in all an ...</td>\n",
       "      <td>[[('Had', 'VBD'), ('the', 'DT'), ('rebus', 'NN...</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.929509</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.070491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8892</th>\n",
       "      <td>Shami Banana Leaf</td>\n",
       "      <td>5</td>\n",
       "      <td>MY FAVOURITE INDIAN RESTAURANT.\\r\\nTheir soya,...</td>\n",
       "      <td>1</td>\n",
       "      <td>MY RESTAURANT . Their soya , sambal &amp; potato a...</td>\n",
       "      <td>[[('MY', 'PRP$'), ('RESTAURANT', 'NNP'), ('.',...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8893</th>\n",
       "      <td>Rahim Muslim Food</td>\n",
       "      <td>4</td>\n",
       "      <td>The taste is unlike the usual Mee rebus you fi...</td>\n",
       "      <td>1</td>\n",
       "      <td>The taste is unlike the usual rebus you find e...</td>\n",
       "      <td>[[('The', 'DT'), ('taste', 'NN'), ('is', 'VBZ'...</td>\n",
       "      <td>0.570238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8894</th>\n",
       "      <td>The Teochew Kitchenette</td>\n",
       "      <td>5</td>\n",
       "      <td>Mixed fish soup (default comes with milk) was ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Mixed fish soup ( default come with milk ) wa ...</td>\n",
       "      <td>[[('Mixed', 'JJ'), ('fish', 'NN'), ('soup', 'N...</td>\n",
       "      <td>0.629877</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8895 rows √ó 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                restaurant name  rating  \\\n",
       "0              TungLok Teahouse       5   \n",
       "1                Malaysia Boleh       5   \n",
       "2                 Douraku Sushi       5   \n",
       "3              511 Indian Store       5   \n",
       "4       The Teochew Kitchenette       4   \n",
       "...                         ...     ...   \n",
       "8890  Ichikokudo Hokkaido Ramen       4   \n",
       "8891      Kedai Makan Muhajirin       4   \n",
       "8892          Shami Banana Leaf       5   \n",
       "8893          Rahim Muslim Food       4   \n",
       "8894    The Teochew Kitchenette       5   \n",
       "\n",
       "                                                content  label  \\\n",
       "0     Lisa is a very good host and made us feel very...      1   \n",
       "1     Very nice piping hot claypot with dark sauce a...      1   \n",
       "2     Such luxuriously enjoyable Omakase ü§ó\\r\\nRich s...      1   \n",
       "3                          Nice India ingredients foods      1   \n",
       "4     Ordered the Stir-Fry Kang Kong and Marmite Chi...      1   \n",
       "...                                                 ...    ...   \n",
       "8890  Wasn‚Äôt crowded during the dinner period.\\r\\nDe...      1   \n",
       "8891  Had the mee rebus, mee siam and nasi lemak wit...      1   \n",
       "8892  MY FAVOURITE INDIAN RESTAURANT.\\r\\nTheir soya,...      1   \n",
       "8893  The taste is unlike the usual Mee rebus you fi...      1   \n",
       "8894  Mixed fish soup (default comes with milk) was ...      1   \n",
       "\n",
       "                                          content_clean  \\\n",
       "0     is a very good host and made u feel very welco...   \n",
       "1     Very nice piping hot with dark sauce and sesam...   \n",
       "2     Such luxuriously enjoyable hugging face Rich s...   \n",
       "3                                  Nice ingredient food   \n",
       "4     Ordered the Stir-Fry Kang and Marmite Chicken ...   \n",
       "...                                                 ...   \n",
       "8890  ‚Äô t crowded during the dinner period . Decent ...   \n",
       "8891  Had the rebus , and nasi with , all in all an ...   \n",
       "8892  MY RESTAURANT . Their soya , sambal & potato a...   \n",
       "8893  The taste is unlike the usual rebus you find e...   \n",
       "8894  Mixed fish soup ( default come with milk ) wa ...   \n",
       "\n",
       "                                              Tokenized  \\\n",
       "0     [[('is', 'VBZ'), ('a', 'DT'), ('very', 'RB'), ...   \n",
       "1     [[('Very', 'RB'), ('nice', 'JJ'), ('piping', '...   \n",
       "2     [[('Such', 'JJ'), ('luxuriously', 'RB'), ('enj...   \n",
       "3     [[('Nice', 'NNP'), ('ingredient', 'NN'), ('foo...   \n",
       "4     [[('Ordered', 'VBN'), ('the', 'DT'), ('Stir-Fr...   \n",
       "...                                                 ...   \n",
       "8890  [[('‚Äô', 'JJ'), ('t', 'NN'), ('crowded', 'VBD')...   \n",
       "8891  [[('Had', 'VBD'), ('the', 'DT'), ('rebus', 'NN...   \n",
       "8892  [[('MY', 'PRP$'), ('RESTAURANT', 'NNP'), ('.',...   \n",
       "8893  [[('The', 'DT'), ('taste', 'NN'), ('is', 'VBZ'...   \n",
       "8894  [[('Mixed', 'JJ'), ('fish', 'NN'), ('soup', 'N...   \n",
       "\n",
       "      predicted_subjectivity  label_pred  score_pred  label_other  score_other  \n",
       "0                   0.468571         1.0    0.999874          0.0     0.000126  \n",
       "1                   0.725714         1.0    0.999867          0.0     0.000133  \n",
       "2                   0.583333         1.0    0.999831          0.0     0.000169  \n",
       "3                   1.000000         1.0    0.996541          0.0     0.003459  \n",
       "4                   0.742857         1.0    0.997530          0.0     0.002470  \n",
       "...                      ...         ...         ...          ...          ...  \n",
       "8890                0.697778         1.0    0.936226          0.0     0.063774  \n",
       "8891                0.675000         1.0    0.929509          0.0     0.070491  \n",
       "8892                0.700000         1.0    0.999437          0.0     0.000563  \n",
       "8893                0.570238         1.0    0.999422          0.0     0.000578  \n",
       "8894                0.629877         1.0    0.999725          0.0     0.000275  \n",
       "\n",
       "[8895 rows x 11 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine back\n",
    "combine_result = pd.concat([backupdf,  outputdf], axis=1)\n",
    "combine_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8abf78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.905\n",
      "Precision: 0.929\n",
      "Recall: 0.905\n",
      "F1: 0.911\n"
     ]
    }
   ],
   "source": [
    "# get metric scores\n",
    "print('Accuracy: %.3f' % accuracy_score(combine_result['label'], combine_result['label_pred']))\n",
    "print('Precision: %.3f' % precision_score(combine_result['label'], combine_result['label_pred'], average=\"weighted\"))\n",
    "print('Recall: %.3f' % recall_score(combine_result['label'], combine_result['label_pred'], average=\"weighted\"))\n",
    "print('F1: %.3f' % f1_score(combine_result['label'], combine_result['label_pred'], average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ac63e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9724\\1712907695.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  single_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\", \"score_pred\": \"score\"}, inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9724\\1712907695.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  double_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#save to csv\n",
    "#version 1 (text, label, score)\n",
    "single_score_version = combine_result[['content_clean', 'label_pred', 'score_pred']]\n",
    "single_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\", \"score_pred\": \"score\"}, inplace=True)\n",
    "single_score_version.to_csv(\"DistilBert_pred_unlabelled10k_single.csv\", index=False)\n",
    "\n",
    "#version 2 (text, label, score_pred, score_other)\n",
    "double_score_version = combine_result[['content_clean', 'label_pred', 'score_pred', 'score_other']]\n",
    "double_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\"}, inplace=True)\n",
    "double_score_version.to_csv(\"DistilBert_pred_unlabelled10k_double.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7dde6051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (8895, 7)\n",
      "single_score_version: (8895, 3)\n",
      "double_score_version: (8895, 4)\n"
     ]
    }
   ],
   "source": [
    "#just make sure the shapes same as original set\n",
    "print(\"original: \" + str(backupdf.shape))\n",
    "print(\"single_score_version: \" + str(single_score_version.shape))\n",
    "print(\"double_score_version: \" + str(double_score_version.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4930635f",
   "metadata": {},
   "source": [
    "### Test on yelp_review_after_subjectivity_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1aeba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-87a8e87d5d978e11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/Admin/.cache/huggingface/datasets/csv/default-87a8e87d5d978e11/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc77b63e66ee4a0486da07b6295cdeb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b6aa7559934338baecfad36f3e43ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Desktop\\NLP temp\\Model\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:697: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/Admin/.cache/huggingface/datasets/csv/default-87a8e87d5d978e11/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf741279d6e4b6d96ac0a8cef944949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load\n",
    "test_dataset = load_dataset('csv', data_files=\"..\\\\..\\\\scraper\\\\data\\\\yelp_review_after_subjectivity_classification.csv\")\n",
    "\n",
    "def tokenizeFunction(examples):\n",
    "    return newTokenizer(examples[\"text\"], max_length=550, padding=\"max_length\", truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce02cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hold a copy as df for later\n",
    "backupdf = pd.DataFrame(test_dataset['train'])\n",
    "\n",
    "#get only the input data\n",
    "test_dataset = test_dataset.remove_columns('Tokenized')\n",
    "test_dataset = test_dataset.remove_columns('predicted_subjectivity')\n",
    "test_dataset = test_dataset.remove_columns('label')\n",
    "test_dataset = test_dataset.remove_columns('content')\n",
    "test_dataset = test_dataset.rename_column('content_clean', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5baf04a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4bebf5263c4b82982124832f75f6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49639 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#need to trunctate as model only handles max 512 tensors\n",
    "def trimNtrunc(examples):\n",
    "    try:\n",
    "        examples['text'] = examples['text'].replace(r'\\n', '')\n",
    "        if len(examples['text']) > 512:\n",
    "            examples['text'] = examples['text'][:512]\n",
    "    except:\n",
    "        examples['text'] = ''\n",
    "        pass\n",
    "    return examples\n",
    "\n",
    "test_dataset = test_dataset['train'].map(trimNtrunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5682eca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdc9fa02af74661b2f96c917d513d6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize the datasets\n",
    "tokenized_dataset = test_dataset.map(tokenizeFunction, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "feace850",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#run thru the model and get output\n",
    "output = sentiment_model(tokenized_dataset['text'], top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "402edbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some magic to format the data\n",
    "outputdf = pd.DataFrame(list(chain.from_iterable(output)))\n",
    "evendf = outputdf.iloc[0:len(outputdf):2]\n",
    "odddf = outputdf.iloc[1:len(outputdf):2]\n",
    "evendf.reset_index(drop=True, inplace=True)\n",
    "odddf.reset_index(drop=True, inplace=True)\n",
    "outputdf = evendf.join(odddf, lsuffix=\"_pred\", rsuffix=\"_other\")\n",
    "\n",
    "outputdf['label_pred'] = outputdf['label_pred'].replace({'LABEL_0':0.0, 'LABEL_1':1.0})\n",
    "outputdf['label_other'] = outputdf['label_other'].replace({'LABEL_0':0.0, 'LABEL_1':1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbd18b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>predicted_subjectivity</th>\n",
       "      <th>label_pred</th>\n",
       "      <th>score_pred</th>\n",
       "      <th>label_other</th>\n",
       "      <th>score_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tried to give this place a second chance and w...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tried to give this place a second chance and w...</td>\n",
       "      <td>[[('Tried', 'VBN'), ('to', 'TO'), ('give', 'VB...</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999384</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My Mom ordered penne pasta and received taglia...</td>\n",
       "      <td>0</td>\n",
       "      <td>My ordered and received instead Delivery perso...</td>\n",
       "      <td>[[('My', 'PRP$'), ('ordered', 'JJ'), ('and', '...</td>\n",
       "      <td>0.688889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999344</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The facility is clean and level however the st...</td>\n",
       "      <td>0</td>\n",
       "      <td>The facility is clean and level however the st...</td>\n",
       "      <td>[[('The', 'DT'), ('facility', 'NN'), ('is', 'V...</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.997516</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Absolutely do not bother There is a coffee bar...</td>\n",
       "      <td>0</td>\n",
       "      <td>Absolutely do not bother There is a coffee bar...</td>\n",
       "      <td>[[('Absolutely', 'RB'), ('do', 'VBP'), ('not',...</td>\n",
       "      <td>0.476389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999438</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inga is the only competent employee here The e...</td>\n",
       "      <td>0</td>\n",
       "      <td>is the only competent employee here The evenin...</td>\n",
       "      <td>[[('is', 'VBZ'), ('the', 'DT'), ('only', 'JJ')...</td>\n",
       "      <td>0.594728</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998617</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49634</th>\n",
       "      <td>Wow  Talk about your dichotomy of the absolute...</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow Talk about your dichotomy of the absolutel...</td>\n",
       "      <td>[[('Wow', 'NNP'), ('Talk', 'VBP'), ('about', '...</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996275</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49635</th>\n",
       "      <td>Awesome location right on the water Great beac...</td>\n",
       "      <td>1</td>\n",
       "      <td>Awesome location right on the water Great beac...</td>\n",
       "      <td>[[('Awesome', 'NNP'), ('location', 'NN'), ('ri...</td>\n",
       "      <td>0.672619</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49636</th>\n",
       "      <td>We had the Ropa Vieja and Pork Chop Chuletas F...</td>\n",
       "      <td>1</td>\n",
       "      <td>We had the and Pork Chop both were really real...</td>\n",
       "      <td>[[('We', 'PRP'), ('had', 'VBD'), ('the', 'DT')...</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999901</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49637</th>\n",
       "      <td>I really enjoyed the place Its small but intim...</td>\n",
       "      <td>1</td>\n",
       "      <td>I really the place Its small but intimate Grea...</td>\n",
       "      <td>[[('I', 'PRP'), ('really', 'RB'), ('the', 'DT'...</td>\n",
       "      <td>0.568519</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.999889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49638</th>\n",
       "      <td>The quest for the best Cuban Sandwich continue...</td>\n",
       "      <td>1</td>\n",
       "      <td>The quest for the best Sandwich for those of y...</td>\n",
       "      <td>[[('The', 'DT'), ('quest', 'JJS'), ('for', 'IN...</td>\n",
       "      <td>0.516758</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.681337</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49639 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  label  \\\n",
       "0      Tried to give this place a second chance and w...      0   \n",
       "1      My Mom ordered penne pasta and received taglia...      0   \n",
       "2      The facility is clean and level however the st...      0   \n",
       "3      Absolutely do not bother There is a coffee bar...      0   \n",
       "4      Inga is the only competent employee here The e...      0   \n",
       "...                                                  ...    ...   \n",
       "49634  Wow  Talk about your dichotomy of the absolute...      1   \n",
       "49635  Awesome location right on the water Great beac...      1   \n",
       "49636  We had the Ropa Vieja and Pork Chop Chuletas F...      1   \n",
       "49637  I really enjoyed the place Its small but intim...      1   \n",
       "49638  The quest for the best Cuban Sandwich continue...      1   \n",
       "\n",
       "                                           content_clean  \\\n",
       "0      Tried to give this place a second chance and w...   \n",
       "1      My ordered and received instead Delivery perso...   \n",
       "2      The facility is clean and level however the st...   \n",
       "3      Absolutely do not bother There is a coffee bar...   \n",
       "4      is the only competent employee here The evenin...   \n",
       "...                                                  ...   \n",
       "49634  Wow Talk about your dichotomy of the absolutel...   \n",
       "49635  Awesome location right on the water Great beac...   \n",
       "49636  We had the and Pork Chop both were really real...   \n",
       "49637  I really the place Its small but intimate Grea...   \n",
       "49638  The quest for the best Sandwich for those of y...   \n",
       "\n",
       "                                               Tokenized  \\\n",
       "0      [[('Tried', 'VBN'), ('to', 'TO'), ('give', 'VB...   \n",
       "1      [[('My', 'PRP$'), ('ordered', 'JJ'), ('and', '...   \n",
       "2      [[('The', 'DT'), ('facility', 'NN'), ('is', 'V...   \n",
       "3      [[('Absolutely', 'RB'), ('do', 'VBP'), ('not',...   \n",
       "4      [[('is', 'VBZ'), ('the', 'DT'), ('only', 'JJ')...   \n",
       "...                                                  ...   \n",
       "49634  [[('Wow', 'NNP'), ('Talk', 'VBP'), ('about', '...   \n",
       "49635  [[('Awesome', 'NNP'), ('location', 'NN'), ('ri...   \n",
       "49636  [[('We', 'PRP'), ('had', 'VBD'), ('the', 'DT')...   \n",
       "49637  [[('I', 'PRP'), ('really', 'RB'), ('the', 'DT'...   \n",
       "49638  [[('The', 'DT'), ('quest', 'JJS'), ('for', 'IN...   \n",
       "\n",
       "       predicted_subjectivity  label_pred  score_pred  label_other  \\\n",
       "0                    0.214286         0.0    0.999384          1.0   \n",
       "1                    0.688889         0.0    0.999344          1.0   \n",
       "2                    0.433333         0.0    0.997516          1.0   \n",
       "3                    0.476389         0.0    0.999438          1.0   \n",
       "4                    0.594728         0.0    0.998617          1.0   \n",
       "...                       ...         ...         ...          ...   \n",
       "49634                0.566667         0.0    0.996275          1.0   \n",
       "49635                0.672619         1.0    0.999306          0.0   \n",
       "49636                0.550000         1.0    0.999901          0.0   \n",
       "49637                0.568519         1.0    0.999889          0.0   \n",
       "49638                0.516758         1.0    0.681337          0.0   \n",
       "\n",
       "       score_other  \n",
       "0         0.000616  \n",
       "1         0.000656  \n",
       "2         0.002484  \n",
       "3         0.000562  \n",
       "4         0.001383  \n",
       "...            ...  \n",
       "49634     0.003725  \n",
       "49635     0.000694  \n",
       "49636     0.000099  \n",
       "49637     0.000111  \n",
       "49638     0.318663  \n",
       "\n",
       "[49639 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#combine back\n",
    "combine_result = pd.concat([backupdf,  outputdf], axis=1)\n",
    "combine_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f922866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.961\n",
      "Precision: 0.961\n",
      "Recall: 0.961\n",
      "F1: 0.961\n"
     ]
    }
   ],
   "source": [
    "# get metric scores\n",
    "print('Accuracy: %.3f' % accuracy_score(combine_result['label'], combine_result['label_pred']))\n",
    "print('Precision: %.3f' % precision_score(combine_result['label'], combine_result['label_pred'], average=\"weighted\"))\n",
    "print('Recall: %.3f' % recall_score(combine_result['label'], combine_result['label_pred'], average=\"weighted\"))\n",
    "print('F1: %.3f' % f1_score(combine_result['label'], combine_result['label_pred'], average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bce9d04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9724\\3708105759.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  single_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\", \"score_pred\": \"score\"}, inplace=True)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9724\\3708105759.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  double_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#save to csv\n",
    "#version 1 (text, label, score)\n",
    "single_score_version = combine_result[['content_clean', 'label_pred', 'score_pred']]\n",
    "single_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\", \"score_pred\": \"score\"}, inplace=True)\n",
    "single_score_version.to_csv(\"DistilBert_pred_yelp50k_single.csv\", index=False)\n",
    "\n",
    "#version 2 (text, label, score_pred, score_other)\n",
    "double_score_version = combine_result[['content_clean', 'label_pred', 'score_pred', 'score_other']]\n",
    "double_score_version.rename(columns={\"content_clean\": \"text\", \"label_pred\" : \"label\"}, inplace=True)\n",
    "double_score_version.to_csv(\"DistilBert_pred_yelp50k_double.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "498b62d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: (49639, 5)\n",
      "single_score_version: (49639, 3)\n",
      "double_score_version: (49639, 4)\n"
     ]
    }
   ],
   "source": [
    "#just make sure the shapes same as original set\n",
    "print(\"original: \" + str(backupdf.shape))\n",
    "print(\"single_score_version: \" + str(single_score_version.shape))\n",
    "print(\"double_score_version: \" + str(double_score_version.shape))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

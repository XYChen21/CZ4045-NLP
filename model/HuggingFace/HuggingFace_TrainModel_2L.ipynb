{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeea640b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dependencies\n",
    "#!pip install -q transformers\n",
    "#!pip install pandas\n",
    "#!pip install sklearn\n",
    "#!pip install torch\n",
    "#!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "#!pip install datasets\n",
    "#!pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fc2a0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c15812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from evaluate import evaluator\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c86cbd",
   "metadata": {},
   "source": [
    "### Login Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e05a652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to C:\\Users\\Admin/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665030f7",
   "metadata": {},
   "source": [
    "## Load & Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68121d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset and process it for training\n",
    "datadf = pd.read_csv(\"..\\\\..\\\\scraper\\\\data\\\\yelp_review_after_subjectivity_classification.csv\")\n",
    "\n",
    "\n",
    "datadf.drop('content', axis=1, inplace=True)\n",
    "datadf.drop('predicted_subjectivity', axis=1, inplace=True)\n",
    "datadf.drop('Tokenized', axis=1, inplace=True)\n",
    "datadf.rename(columns={\"content_clean\": \"text\"}, inplace=True)\n",
    "datadf = datadf[['text', 'label']]\n",
    "\n",
    "datadf.to_csv(\"yelp_review_processed_2L.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbca428",
   "metadata": {},
   "source": [
    "### Test using pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c15215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6e939f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba59281f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fb394bbe0e22a1f9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:/Users/Admin/.cache/huggingface/datasets/csv/default-fb394bbe0e22a1f9/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ee7f029e474c769e28af67fe7ff0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08f5343fe9864855baced8cd9649f453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Desktop\\NLP temp\\Model\\venv\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py:697: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/Admin/.cache/huggingface/datasets/csv/default-fb394bbe0e22a1f9/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eabf958c267473889a700648b201fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loadDataset = load_dataset('csv', data_files=\"yelp_review_processed_2L.csv\")\n",
    "dataEval = loadDataset['train']\n",
    "\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "\n",
    "evalResults = task_evaluator.compute(\n",
    "    model_or_pipeline=model,\n",
    "    tokenizer=tokenizer,\n",
    "    data=dataEval,\n",
    "    metric=evaluate.combine([\"accuracy\", \"precision\", \"recall\", \"f1\"]),\n",
    "    label_mapping={\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d22cd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8796913717037007, 'precision': 0.9460922787193974, 'recall': 0.8063157049995987, 'f1': 0.8706295221177591, 'total_time_in_seconds': 212.6748084, 'samples_per_second': 233.40329009084465, 'latency_in_seconds': 0.004284429750800781}\n"
     ]
    }
   ],
   "source": [
    "print(evalResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8839408",
   "metadata": {},
   "source": [
    "## Fine Tune model with Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbf488a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "# traintest = load_dataset('csv', data_files=\"yelp_review_processed.csv\")\n",
    "# # split into train and test, 80% - 20%\n",
    "# train_test_split = traintest[\"train\"].train_test_split(train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92cd9a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# def tokenizeFunction(examples):\n",
    "#     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03930677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #tokenize the datasets\n",
    "# tok_train = train_test_split[\"train\"].map(tokenizeFunction, batched=True)\n",
    "# tok_test = train_test_split[\"test\"].map(tokenizeFunction, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a6354a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train using pytorch trainer api, with 2 labels, Positive & Negative\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2cbab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #training arguments\n",
    "# batch_size = 16\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir = \"sentiment-fine-tuned-yelp-3L\",\n",
    "#     overwrite_output_dir = True,\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     learning_rate = 2e-5,\n",
    "#     weight_decay = 0.01,\n",
    "#     per_device_train_batch_size = batch_size,\n",
    "#     per_device_eval_batch_size = batch_size,\n",
    "#     logging_steps = len(tok_train) // batch_size,\n",
    "#     push_to_hub = True,\n",
    "# )\n",
    "\n",
    "# #compute metrics function\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# #trainer\n",
    "# trainer = Trainer(\n",
    "#    model = model,\n",
    "#    tokenizer = tokenizer,\n",
    "#    args = training_args,\n",
    "#    train_dataset = tok_train,\n",
    "#    eval_dataset = tok_test,\n",
    "#    compute_metrics = compute_metrics,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7368278e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #perform the fine tuning with trainer\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5a7c639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424739d1",
   "metadata": {},
   "source": [
    "## Fine Tune model using pytorch native method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f1fa10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-fb394bbe0e22a1f9\n",
      "Found cached dataset csv (C:/Users/Admin/.cache/huggingface/datasets/csv/default-fb394bbe0e22a1f9/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fc44092cc81418f814f64d1e399e037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18a2d9df48d450dadc72773ba67b5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75ceedc7e8941fa96e7866e37a5feff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler, AutoTokenizer\n",
    "\n",
    "traintest = load_dataset('csv', data_files=\"yelp_review_processed_2L.csv\")\n",
    "# split into train and test, 80% - 20%\n",
    "train_test_split = traintest[\"train\"].train_test_split(train_size=0.8)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenizeFunction(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "#tokenize the datasets\n",
    "tok_train = train_test_split[\"train\"].map(tokenizeFunction, batched=True)\n",
    "tok_test = train_test_split[\"test\"].map(tokenizeFunction, batched=True)\n",
    "\n",
    "#prepare for torch\n",
    "tok_train = tok_train.remove_columns([\"text\"])\n",
    "tok_test = tok_test.remove_columns([\"text\"])\n",
    "tok_train = tok_train.rename_column(\"label\", \"labels\")\n",
    "tok_test = tok_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "tok_train.set_format(\"torch\")\n",
    "tok_test.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(tok_train, shuffle=True, batch_size=32)\n",
    "eval_dataloader = DataLoader(tok_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b05e93ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#train using pytorch, with 2 labels, Positive & Negative\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e816bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "702d032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "num_train_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name = \"linear\", optimizer = optimizer, num_warmup_steps = 0, num_training_steps = num_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33552cfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1aca9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e34df0d13f4203ad85625ab06c4f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3723 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_train_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2af00c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9367445608380338}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8fff52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"sentiment-fine-tuned-yelp-2L\")\n",
    "tokenizer.push_to_hub(\"sentiment-fine-tuned-yelp-2L\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540f640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100ac7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156e59b",
   "metadata": {},
   "source": [
    "### Import scrapped data and drop unnessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82cb9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index(['restaurant name', 'rating', 'content', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_10k = pd.read_csv('data_all(10881).xlsx - Sheet1.csv')  \n",
    "print(type(df_10k))\n",
    "df_10k.drop(columns=['uid', 'Label #1', 'Label #2', 'Label #3'], inplace=True)\n",
    "df_10k.columns = ['restaurant name', 'rating', 'content', 'label']\n",
    "print(df_10k.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ecfa76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant name</th>\n",
       "      <th>rating</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Konomi Zen</td>\n",
       "      <td>3</td>\n",
       "      <td>Crunchy tempura esp the vegetables</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vincent Western Food</td>\n",
       "      <td>5</td>\n",
       "      <td>this is one of the best western food i've eate...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siam Square Mookata - Best Mookata Restaurant ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Many choice of food to select. Love their teri...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Old Chang Kee</td>\n",
       "      <td>1</td>\n",
       "      <td>Buying snacks for customers but system mainten...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiang Ji Cantonese Roasts</td>\n",
       "      <td>1</td>\n",
       "      <td>Seriously overprice and rude service. Avoid at...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>Ichikokudo Hokkaido Ramen</td>\n",
       "      <td>4</td>\n",
       "      <td>Wasnâ€™t crowded during the dinner period.\\nDece...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10113</th>\n",
       "      <td>Kedai Makan Muhajirin</td>\n",
       "      <td>4</td>\n",
       "      <td>Had the mee rebus, mee siam and nasi lemak wit...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>Shami Banana Leaf</td>\n",
       "      <td>5</td>\n",
       "      <td>MY FAVOURITE INDIAN RESTAURANT.\\nTheir soya, T...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>Rahim Muslim Food</td>\n",
       "      <td>4</td>\n",
       "      <td>The taste is unlike the usual Mee rebus you fi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10116</th>\n",
       "      <td>The Teochew Kitchenette</td>\n",
       "      <td>5</td>\n",
       "      <td>Mixed fish soup (default comes with milk) was ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10117 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         restaurant name  rating  \\\n",
       "0                                             Konomi Zen       3   \n",
       "1                                   Vincent Western Food       5   \n",
       "2      Siam Square Mookata - Best Mookata Restaurant ...       5   \n",
       "3                                          Old Chang Kee       1   \n",
       "4                              Hiang Ji Cantonese Roasts       1   \n",
       "...                                                  ...     ...   \n",
       "10112                          Ichikokudo Hokkaido Ramen       4   \n",
       "10113                              Kedai Makan Muhajirin       4   \n",
       "10114                                  Shami Banana Leaf       5   \n",
       "10115                                  Rahim Muslim Food       4   \n",
       "10116                            The Teochew Kitchenette       5   \n",
       "\n",
       "                                                 content  label  \n",
       "0                     Crunchy tempura esp the vegetables    2.0  \n",
       "1      this is one of the best western food i've eate...    1.0  \n",
       "2      Many choice of food to select. Love their teri...    1.0  \n",
       "3      Buying snacks for customers but system mainten...    0.0  \n",
       "4      Seriously overprice and rude service. Avoid at...    0.0  \n",
       "...                                                  ...    ...  \n",
       "10112  Wasnâ€™t crowded during the dinner period.\\nDece...    NaN  \n",
       "10113  Had the mee rebus, mee siam and nasi lemak wit...    NaN  \n",
       "10114  MY FAVOURITE INDIAN RESTAURANT.\\nTheir soya, T...    NaN  \n",
       "10115  The taste is unlike the usual Mee rebus you fi...    NaN  \n",
       "10116  Mixed fish soup (default comes with milk) was ...    NaN  \n",
       "\n",
       "[10117 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_10k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17136c9",
   "metadata": {},
   "source": [
    "### Cleaning Content with Emoji Removal, Lemmatizer and Non English words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "216e315a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flashtext\n",
    "#!pip install emot\n",
    "import emot\n",
    "from emot.emo_unicode import UNICODE_EMOJI, UNICODE_EMOJI_ALIAS, EMOTICONS_EMO\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "## formatting\n",
    "all_emoji_emoticons = {**EMOTICONS_EMO,**UNICODE_EMOJI_ALIAS, **UNICODE_EMOJI_ALIAS}\n",
    "all_emoji_emoticons = {k:v.replace(\":\",\"\").replace(\"_\",\" \").strip() for k,v in all_emoji_emoticons.items()}\n",
    "\n",
    "kp_all_emoji_emoticons = KeywordProcessor()\n",
    "for k,v in all_emoji_emoticons.items():\n",
    "    kp_all_emoji_emoticons.add_keyword(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d00fd17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "C:\\Users\\lauka\\AppData\\Local\\Temp\\ipykernel_35900\\2413352189.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_10k['content_clean'][index] = row['content_clean']\n",
      "C:\\Users\\lauka\\AppData\\Local\\Temp\\ipykernel_35900\\2413352189.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_10k['Tokenized'][index] = row['Tokenized']\n"
     ]
    }
   ],
   "source": [
    "# Might take awhile to run if dataset is large as it iterates through every row\n",
    "\n",
    "#nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "#!pip install clean-text\n",
    "from cleantext import clean\n",
    "\n",
    "#nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_10k['content_clean'] = \"\"\n",
    "df_10k['Tokenized'] = \"\"\n",
    "\n",
    "for index, row in df_10k.iterrows():\n",
    "    # removes all emoji\n",
    "    #row['content_clean'] = clean(row['content'],no_emoji=True)\n",
    "    \n",
    "    # Lemmatize the words in sentence\n",
    "    tokenized_text = word_tokenize(row['content'])\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
    "\n",
    "    \"\"\" \n",
    "    Removes non english words by:\n",
    "    Joining English words w.lower() in words and joins with symbols/punctation --> w.alpha()\n",
    "    Limitations :\n",
    "    Removes some words:\n",
    "    1. NER nouns (teriyaki chicken becomes chicken)\n",
    "    2. Mispelled\n",
    "    3. Split sort forms like can't , i've (i've become i' because ve is not a word)\n",
    "    \"\"\" # Remove Non English word in nltk.corpus\n",
    "    row['content_clean'] = \" \".join(w for w in lemmatized_text if w.lower() in words or not w.isalpha())\n",
    "\n",
    "    # Replacing emoji with words instead, done after because i want to retain the full text of emoji\n",
    "    row['content_clean'] = kp_all_emoji_emoticons.replace_keywords(row['content_clean'])\n",
    "    \n",
    "    # Tokenize each sentence\n",
    "    row['Tokenized'] = [ word_tokenize(t) for t in sent_tokenize(row['content_clean']) if t not in words ]\n",
    "    \n",
    "    # Pos tagging for each sentence in row['Tokenized']\n",
    "    for i in range (len(row['Tokenized'])):\n",
    "        # i refers to each of the tokenized sentence\n",
    "        row['Tokenized'][i] = pos_tag(row['Tokenized'][i])\n",
    "    \n",
    "    df_10k['content_clean'][index] = row['content_clean']\n",
    "    df_10k['Tokenized'][index] = row['Tokenized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2391d9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant name</th>\n",
       "      <th>rating</th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>Tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Konomi Zen</td>\n",
       "      <td>3</td>\n",
       "      <td>Crunchy tempura esp the vegetables</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Crunchy the vegetable</td>\n",
       "      <td>[[(Crunchy, NNP), (the, DT), (vegetable, NN)]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vincent Western Food</td>\n",
       "      <td>5</td>\n",
       "      <td>this is one of the best western food i've eate...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>this is one of the best western food i 've eat...</td>\n",
       "      <td>[[(this, DT), (is, VBZ), (one, CD), (of, IN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siam Square Mookata - Best Mookata Restaurant ...</td>\n",
       "      <td>5</td>\n",
       "      <td>Many choice of food to select. Love their teri...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Many choice of food to select . Love their por...</td>\n",
       "      <td>[[(Many, JJ), (choice, NN), (of, IN), (food, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Old Chang Kee</td>\n",
       "      <td>1</td>\n",
       "      <td>Buying snacks for customers but system mainten...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>snack for customer but system maintenance cant...</td>\n",
       "      <td>[[(snack, NN), (for, IN), (customer, NN), (but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiang Ji Cantonese Roasts</td>\n",
       "      <td>1</td>\n",
       "      <td>Seriously overprice and rude service. Avoid at...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Seriously overprice and rude service . Avoid a...</td>\n",
       "      <td>[[(Seriously, RB), (overprice, NN), (and, CC),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10112</th>\n",
       "      <td>Ichikokudo Hokkaido Ramen</td>\n",
       "      <td>4</td>\n",
       "      <td>Wasnâ€™t crowded during the dinner period.\\nDece...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>â€™ t crowded during the dinner period . Decent ...</td>\n",
       "      <td>[[(â€™, JJ), (t, NN), (crowded, VBD), (during, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10113</th>\n",
       "      <td>Kedai Makan Muhajirin</td>\n",
       "      <td>4</td>\n",
       "      <td>Had the mee rebus, mee siam and nasi lemak wit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Had the rebus , and nasi with , all in all an ...</td>\n",
       "      <td>[[(Had, VBD), (the, DT), (rebus, NN), (,, ,), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10114</th>\n",
       "      <td>Shami Banana Leaf</td>\n",
       "      <td>5</td>\n",
       "      <td>MY FAVOURITE INDIAN RESTAURANT.\\nTheir soya, T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MY RESTAURANT . Their soya , sambal &amp; potato a...</td>\n",
       "      <td>[[(MY, PRP$), (RESTAURANT, NNP), (., .)], [(Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10115</th>\n",
       "      <td>Rahim Muslim Food</td>\n",
       "      <td>4</td>\n",
       "      <td>The taste is unlike the usual Mee rebus you fi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The taste is unlike the usual rebus you find e...</td>\n",
       "      <td>[[(The, DT), (taste, NN), (is, VBZ), (unlike, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10116</th>\n",
       "      <td>The Teochew Kitchenette</td>\n",
       "      <td>5</td>\n",
       "      <td>Mixed fish soup (default comes with milk) was ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mixed fish soup ( default come with milk ) wa ...</td>\n",
       "      <td>[[(Mixed, JJ), (fish, NN), (soup, NN), ((, (),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10117 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         restaurant name  rating  \\\n",
       "0                                             Konomi Zen       3   \n",
       "1                                   Vincent Western Food       5   \n",
       "2      Siam Square Mookata - Best Mookata Restaurant ...       5   \n",
       "3                                          Old Chang Kee       1   \n",
       "4                              Hiang Ji Cantonese Roasts       1   \n",
       "...                                                  ...     ...   \n",
       "10112                          Ichikokudo Hokkaido Ramen       4   \n",
       "10113                              Kedai Makan Muhajirin       4   \n",
       "10114                                  Shami Banana Leaf       5   \n",
       "10115                                  Rahim Muslim Food       4   \n",
       "10116                            The Teochew Kitchenette       5   \n",
       "\n",
       "                                                 content  label  \\\n",
       "0                     Crunchy tempura esp the vegetables    2.0   \n",
       "1      this is one of the best western food i've eate...    1.0   \n",
       "2      Many choice of food to select. Love their teri...    1.0   \n",
       "3      Buying snacks for customers but system mainten...    0.0   \n",
       "4      Seriously overprice and rude service. Avoid at...    0.0   \n",
       "...                                                  ...    ...   \n",
       "10112  Wasnâ€™t crowded during the dinner period.\\nDece...    NaN   \n",
       "10113  Had the mee rebus, mee siam and nasi lemak wit...    NaN   \n",
       "10114  MY FAVOURITE INDIAN RESTAURANT.\\nTheir soya, T...    NaN   \n",
       "10115  The taste is unlike the usual Mee rebus you fi...    NaN   \n",
       "10116  Mixed fish soup (default comes with milk) was ...    NaN   \n",
       "\n",
       "                                           content_clean  \\\n",
       "0                                  Crunchy the vegetable   \n",
       "1      this is one of the best western food i 've eat...   \n",
       "2      Many choice of food to select . Love their por...   \n",
       "3      snack for customer but system maintenance cant...   \n",
       "4      Seriously overprice and rude service . Avoid a...   \n",
       "...                                                  ...   \n",
       "10112  â€™ t crowded during the dinner period . Decent ...   \n",
       "10113  Had the rebus , and nasi with , all in all an ...   \n",
       "10114  MY RESTAURANT . Their soya , sambal & potato a...   \n",
       "10115  The taste is unlike the usual rebus you find e...   \n",
       "10116  Mixed fish soup ( default come with milk ) wa ...   \n",
       "\n",
       "                                               Tokenized  \n",
       "0         [[(Crunchy, NNP), (the, DT), (vegetable, NN)]]  \n",
       "1      [[(this, DT), (is, VBZ), (one, CD), (of, IN), ...  \n",
       "2      [[(Many, JJ), (choice, NN), (of, IN), (food, N...  \n",
       "3      [[(snack, NN), (for, IN), (customer, NN), (but...  \n",
       "4      [[(Seriously, RB), (overprice, NN), (and, CC),...  \n",
       "...                                                  ...  \n",
       "10112  [[(â€™, JJ), (t, NN), (crowded, VBD), (during, I...  \n",
       "10113  [[(Had, VBD), (the, DT), (rebus, NN), (,, ,), ...  \n",
       "10114  [[(MY, PRP$), (RESTAURANT, NNP), (., .)], [(Th...  \n",
       "10115  [[(The, DT), (taste, NN), (is, VBZ), (unlike, ...  \n",
       "10116  [[(Mixed, JJ), (fish, NN), (soup, NN), ((, (),...  \n",
       "\n",
       "[10117 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdce8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Such luxuriously enjoyable Omakase ðŸ¤—\n",
      "Rich sea urchin melts in our mouth ðŸ¤¤\n",
      "With friendly staff and chef, complementing with the sweet umeshu, making the meal even more mesmerising ðŸ¤ \n",
      "Such luxuriously enjoyable hugging face Rich sea urchin melt in our mouth drooling face With friendly staff and chef , with the sweet , making the meal even more cowboy hat face\n"
     ]
    }
   ],
   "source": [
    "print(df_10k['content'][1002])\n",
    "print(df_10k['content_clean'][1002])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe1888f",
   "metadata": {},
   "source": [
    "### Splitting Dataset into Labelled and UnLabelled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "800ceee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = df_10k.loc[df_10k['label'].notna()]\n",
    "df_train = df_10k.loc[df_10k['label'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b938270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 restaurant name  rating  \\\n",
      "1000            TungLok Teahouse       5   \n",
      "1001              Malaysia Boleh       5   \n",
      "1002               Douraku Sushi       5   \n",
      "1003            511 Indian Store       5   \n",
      "1004     The Teochew Kitchenette       4   \n",
      "...                          ...     ...   \n",
      "10112  Ichikokudo Hokkaido Ramen       4   \n",
      "10113      Kedai Makan Muhajirin       4   \n",
      "10114          Shami Banana Leaf       5   \n",
      "10115          Rahim Muslim Food       4   \n",
      "10116    The Teochew Kitchenette       5   \n",
      "\n",
      "                                                 content  label  \\\n",
      "1000   Lisa is a very good host and made us feel very...      1   \n",
      "1001   Very nice piping hot claypot with dark sauce a...      1   \n",
      "1002   Such luxuriously enjoyable Omakase ðŸ¤—\\nRich sea...      1   \n",
      "1003                        Nice India ingredients foods      1   \n",
      "1004   Ordered the Stir-Fry Kang Kong and Marmite Chi...      1   \n",
      "...                                                  ...    ...   \n",
      "10112  Wasnâ€™t crowded during the dinner period.\\nDece...      1   \n",
      "10113  Had the mee rebus, mee siam and nasi lemak wit...      1   \n",
      "10114  MY FAVOURITE INDIAN RESTAURANT.\\nTheir soya, T...      1   \n",
      "10115  The taste is unlike the usual Mee rebus you fi...      1   \n",
      "10116  Mixed fish soup (default comes with milk) was ...      1   \n",
      "\n",
      "                                           content_clean  \\\n",
      "1000   is a very good host and made u feel very welco...   \n",
      "1001   Very nice piping hot with dark sauce and sesam...   \n",
      "1002   Such luxuriously enjoyable hugging face Rich s...   \n",
      "1003                                Nice ingredient food   \n",
      "1004   Ordered the Stir-Fry Kang and Marmite Chicken ...   \n",
      "...                                                  ...   \n",
      "10112  â€™ t crowded during the dinner period . Decent ...   \n",
      "10113  Had the rebus , and nasi with , all in all an ...   \n",
      "10114  MY RESTAURANT . Their soya , sambal & potato a...   \n",
      "10115  The taste is unlike the usual rebus you find e...   \n",
      "10116  Mixed fish soup ( default come with milk ) wa ...   \n",
      "\n",
      "                                               Tokenized  \n",
      "1000   [[(is, VBZ), (a, DT), (very, RB), (good, JJ), ...  \n",
      "1001   [[(Very, RB), (nice, JJ), (piping, VBG), (hot,...  \n",
      "1002   [[(Such, JJ), (luxuriously, RB), (enjoyable, J...  \n",
      "1003       [[(Nice, NNP), (ingredient, NN), (food, NN)]]  \n",
      "1004   [[(Ordered, VBN), (the, DT), (Stir-Fry, NNP), ...  \n",
      "...                                                  ...  \n",
      "10112  [[(â€™, JJ), (t, NN), (crowded, VBD), (during, I...  \n",
      "10113  [[(Had, VBD), (the, DT), (rebus, NN), (,, ,), ...  \n",
      "10114  [[(MY, PRP$), (RESTAURANT, NNP), (., .)], [(Th...  \n",
      "10115  [[(The, DT), (taste, NN), (is, VBZ), (unlike, ...  \n",
      "10116  [[(Mixed, JJ), (fish, NN), (soup, NN), ((, (),...  \n",
      "\n",
      "[9117 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lauka\\AppData\\Local\\Temp\\ipykernel_35900\\2597873921.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['label'] = np.select(conditions,Labels)\n"
     ]
    }
   ],
   "source": [
    "# Assigning Positive and Negative to Train data \n",
    "# <=2 is Negative, Label = 0\n",
    "# >=3 is Positive, Label = 1\n",
    "\n",
    "conditions = [\n",
    "    (df_train['rating'] < 3),\n",
    "    (df_train['rating'] >= 3)\n",
    "    ]\n",
    "Labels = [0, 1]\n",
    "df_train['label'] = np.select(conditions,Labels)\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e86d9c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    7434\n",
       "0    1683\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['label'].value_counts()\n",
    "# if use <= 3 is Negative, ratio is 0.30 : 0.70\n",
    "# if use <= 2 is Negative ratio is 0.18 : 0.82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5f41c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    707\n",
       "0.0    233\n",
       "2.0     60\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval['label'].value_counts()\n",
    "# Ratio of Negative : Positive : Neutral is 0.707 : 0.233 : 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0027f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    56\n",
      "0.0    41\n",
      "2.0    26\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "eval_rating_3 = df_eval.loc[df_eval['rating'] == 3 ]\n",
    "print(eval_rating_3['label'].value_counts())\n",
    "# Ratio of Negative : Positive : Neutral is 0.33 : 0.46 : 0.21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9e926",
   "metadata": {},
   "source": [
    "### Subjectivity Detection Using Textblob\n",
    "\n",
    "#### https://link.springer.com.remotexs.ntu.edu.sg/chapter/10.1007/978-981-15-1884-3_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61f5683b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install TextBlob\n",
    "from textblob import TextBlob\n",
    "def getSubjectivity(text):\n",
    "    #\"Biased\":0, \"Neutral\":1\n",
    "    subj = TextBlob(text).sentiment.subjectivity\n",
    "    # print(f\"debug: subjectivity score is {subj}\")\n",
    "    return subj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f45c0",
   "metadata": {},
   "source": [
    "#### Testing Textblob subjectivity detection with our manual labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e2a2b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lauka\\AppData\\Local\\Temp\\ipykernel_35900\\2079028838.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_eval['predicted_subjectivity'] = df_eval['content_clean'].apply(getSubjectivity)\n",
      "C:\\Users\\lauka\\AppData\\Local\\Temp\\ipykernel_35900\\2079028838.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_train['predicted_subjectivity'] = df_train['content_clean'].apply(getSubjectivity)\n"
     ]
    }
   ],
   "source": [
    "df_eval['predicted_subjectivity'] = df_eval['content_clean'].apply(getSubjectivity)\n",
    "df_train['predicted_subjectivity'] = df_train['content_clean'].apply(getSubjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3412878f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjectivity Detection Accuracy: 93.6\n"
     ]
    }
   ],
   "source": [
    "objective = 0 # Number of Objective/Unbiased Review\n",
    "neutral_count = 0\n",
    "for i in range(len(df_eval)):\n",
    "    \n",
    "    if df_eval['predicted_subjectivity'][i] == 0:\n",
    "        p = 1 # Neutral\n",
    "    else:\n",
    "        p = 0 # Biased\n",
    "    \n",
    "    if df_eval['label'][i] == 2: \n",
    "        label = 1 # Neutral\n",
    "    else:\n",
    "        label = 0 # Biased\n",
    "    \n",
    "    if label != p:\n",
    "        # If there is a mismatch of objective class, add 1 count\n",
    "        objective += 1\n",
    "    #if label == 1 and p == 1:\n",
    "        #neutral_count += 1\n",
    "\n",
    "#print(neutral_count)  # Correctly identify 11 out of 60 Neutral labelled reviews...\n",
    "# Percentage of Objective Review (non opinionated)\n",
    "objective_score = objective/len(df_eval)\n",
    "print('Subjectivity Detection Accuracy:', (1 - objective_score)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "993b4816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Original Labelled Dataset: 1000\n",
      "Length of Labelled Dataset after Textblob Subjectivity: 976 \n",
      "\n",
      "Length of Original UnLabelled Dataset: 9117\n",
      "Length of UnLabelled Dataset after Textblob Subjectivity: 8895\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Original Labelled Dataset:\", len(df_eval))\n",
    "# Drop Predicted value = 0.0000 ... i.e. Neutral Review by Textblob Subjectivity\n",
    "df_eval = df_eval[df_eval.predicted_subjectivity != 0]\n",
    "print(\"Length of Labelled Dataset after Textblob Subjectivity:\", len(df_eval), '\\n')\n",
    "\n",
    "print(\"Length of Original UnLabelled Dataset:\", len(df_train))\n",
    "# Drop Predicted value = 0.0000 ... i.e. Neutral Review by Textblob Subjectivity\n",
    "df_train = df_train[df_train.predicted_subjectivity != 0]\n",
    "print(\"Length of UnLabelled Dataset after Textblob Subjectivity:\", len(df_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e08c2ac",
   "metadata": {},
   "source": [
    "### Saving csv files for scrapped 10k data\n",
    "\n",
    "keeping records if it is opinionated... <br> \n",
    "i.e predicted score for subjectivity > 0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97c185de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.to_csv('labelled_10k.csv',index=False)\n",
    "df_train.to_csv('unlabelled_10k.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaab870",
   "metadata": {},
   "source": [
    "### Yelp dataset Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9da8a8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tried to give this place a second chance and w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My Mom ordered penne pasta and received taglia...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The facility is clean and level however the st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Absolutely do not bother There is a coffee bar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inga is the only competent employee here The e...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Wow  Talk about your dichotomy of the absolute...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Awesome location right on the water Great beac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>We had the Ropa Vieja and Pork Chop Chuletas F...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I really enjoyed the place Its small but intim...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>The quest for the best Cuban Sandwich continue...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  label\n",
       "0      Tried to give this place a second chance and w...      0\n",
       "1      My Mom ordered penne pasta and received taglia...      0\n",
       "2      The facility is clean and level however the st...      0\n",
       "3      Absolutely do not bother There is a coffee bar...      0\n",
       "4      Inga is the only competent employee here The e...      0\n",
       "...                                                  ...    ...\n",
       "49995  Wow  Talk about your dichotomy of the absolute...      1\n",
       "49996  Awesome location right on the water Great beac...      1\n",
       "49997  We had the Ropa Vieja and Pork Chop Chuletas F...      1\n",
       "49998  I really enjoyed the place Its small but intim...      1\n",
       "49999  The quest for the best Cuban Sandwich continue...      1\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp = pd.read_csv('yelp_review_processed_3-5pos.csv')  \n",
    "df_yelp.columns = ['content','label']\n",
    "df_yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3583f51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tried to give this place a second chance and will not be going back Used a Groupon for state inspection They told me I needed 5 bulbs replaced and front rotors replaced to pass inspection and it would be 400 Took my car back to Peruzzi where I purchased the vehicle to have them double check Peruzzi informed me only 2 bulbs needed to be replaced and that the rotors passed but need to be replaced soon Appears Meinke thinks women know nothing about cars and just want to lie and say things need to be replaced ASAP so you think you are forced to use them for repairs for your car will fail state inspection'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13aff50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lauka\\AppData\\Local\\Temp\\ipykernel_35900\\3129739272.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_yelp['content_clean'][index] = row['content_clean']\n",
      "C:\\Users\\lauka\\AppData\\Local\\Temp\\ipykernel_35900\\3129739272.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_yelp['Tokenized'][index] = row['Tokenized']\n"
     ]
    }
   ],
   "source": [
    "# Might take awhile to run if dataset is large as it iterates through every row\n",
    "\n",
    "#nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "#!pip install clean-text\n",
    "from cleantext import clean\n",
    "\n",
    "#nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df_yelp['content_clean'] = \"\"\n",
    "df_yelp['Tokenized'] = \"\"\n",
    "\n",
    "for index, row in df_yelp.iterrows():\n",
    "    # removes all emoji\n",
    "    #row['content_clean'] = clean(row['content'],no_emoji=True)\n",
    "    \n",
    "    # Lemmatize the words in sentence\n",
    "    tokenized_text = word_tokenize(row['content'])\n",
    "    lemmatized_text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
    "\n",
    "    \"\"\" \n",
    "    Removes non english words by:\n",
    "    Joining English words w.lower() in words and joins with symbols/punctation --> w.alpha()\n",
    "    Limitations :\n",
    "    Removes some words:\n",
    "    1. NER nouns (teriyaki chicken becomes chicken)\n",
    "    2. Mispelled\n",
    "    3. Split sort forms like can't , i've (i've become i' because ve is not a word)\n",
    "    \"\"\" # Remove Non English word in nltk.corpus\n",
    "    row['content_clean'] = \" \".join(w for w in lemmatized_text if w.lower() in words or not w.isalpha())\n",
    "\n",
    "    # Replacing emoji with words instead, done after because i want to retain the full text of emoji\n",
    "    row['content_clean'] = kp_all_emoji_emoticons.replace_keywords(row['content_clean'])\n",
    "    \n",
    "    # Tokenize each sentence\n",
    "    row['Tokenized'] = [ word_tokenize(t) for t in sent_tokenize(row['content_clean']) if t not in words ]\n",
    "    \n",
    "    # Pos tagging for each sentence in row['Tokenized']\n",
    "    for i in range (len(row['Tokenized'])):\n",
    "        # i refers to each of the tokenized sentence\n",
    "        row['Tokenized'][i] = pos_tag(row['Tokenized'][i])\n",
    "    \n",
    "    df_yelp['content_clean'][index] = row['content_clean']\n",
    "    df_yelp['Tokenized'][index] = row['Tokenized']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51707b72",
   "metadata": {},
   "source": [
    "### 50k Yelp Textblob subjectivity classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d234c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp['predicted_subjectivity'] = df_yelp['content_clean'].apply(getSubjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04ecffdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>label</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>Tokenized</th>\n",
       "      <th>predicted_subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tried to give this place a second chance and w...</td>\n",
       "      <td>0</td>\n",
       "      <td>Tried to give this place a second chance and w...</td>\n",
       "      <td>[[(Tried, VBN), (to, TO), (give, VB), (this, D...</td>\n",
       "      <td>0.214286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My Mom ordered penne pasta and received taglia...</td>\n",
       "      <td>0</td>\n",
       "      <td>My ordered and received instead Delivery perso...</td>\n",
       "      <td>[[(My, PRP$), (ordered, JJ), (and, CC), (recei...</td>\n",
       "      <td>0.688889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The facility is clean and level however the st...</td>\n",
       "      <td>0</td>\n",
       "      <td>The facility is clean and level however the st...</td>\n",
       "      <td>[[(The, DT), (facility, NN), (is, VBZ), (clean...</td>\n",
       "      <td>0.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Absolutely do not bother There is a coffee bar...</td>\n",
       "      <td>0</td>\n",
       "      <td>Absolutely do not bother There is a coffee bar...</td>\n",
       "      <td>[[(Absolutely, RB), (do, VBP), (not, RB), (bot...</td>\n",
       "      <td>0.476389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Inga is the only competent employee here The e...</td>\n",
       "      <td>0</td>\n",
       "      <td>is the only competent employee here The evenin...</td>\n",
       "      <td>[[(is, VBZ), (the, DT), (only, JJ), (competent...</td>\n",
       "      <td>0.594728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Wow  Talk about your dichotomy of the absolute...</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow Talk about your dichotomy of the absolutel...</td>\n",
       "      <td>[[(Wow, NNP), (Talk, VBP), (about, IN), (your,...</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Awesome location right on the water Great beac...</td>\n",
       "      <td>1</td>\n",
       "      <td>Awesome location right on the water Great beac...</td>\n",
       "      <td>[[(Awesome, NNP), (location, NN), (right, RB),...</td>\n",
       "      <td>0.672619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>We had the Ropa Vieja and Pork Chop Chuletas F...</td>\n",
       "      <td>1</td>\n",
       "      <td>We had the and Pork Chop both were really real...</td>\n",
       "      <td>[[(We, PRP), (had, VBD), (the, DT), (and, CC),...</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I really enjoyed the place Its small but intim...</td>\n",
       "      <td>1</td>\n",
       "      <td>I really the place Its small but intimate Grea...</td>\n",
       "      <td>[[(I, PRP), (really, RB), (the, DT), (place, N...</td>\n",
       "      <td>0.568519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>The quest for the best Cuban Sandwich continue...</td>\n",
       "      <td>1</td>\n",
       "      <td>The quest for the best Sandwich for those of y...</td>\n",
       "      <td>[[(The, DT), (quest, JJS), (for, IN), (the, DT...</td>\n",
       "      <td>0.516758</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  label  \\\n",
       "0      Tried to give this place a second chance and w...      0   \n",
       "1      My Mom ordered penne pasta and received taglia...      0   \n",
       "2      The facility is clean and level however the st...      0   \n",
       "3      Absolutely do not bother There is a coffee bar...      0   \n",
       "4      Inga is the only competent employee here The e...      0   \n",
       "...                                                  ...    ...   \n",
       "49995  Wow  Talk about your dichotomy of the absolute...      1   \n",
       "49996  Awesome location right on the water Great beac...      1   \n",
       "49997  We had the Ropa Vieja and Pork Chop Chuletas F...      1   \n",
       "49998  I really enjoyed the place Its small but intim...      1   \n",
       "49999  The quest for the best Cuban Sandwich continue...      1   \n",
       "\n",
       "                                           content_clean  \\\n",
       "0      Tried to give this place a second chance and w...   \n",
       "1      My ordered and received instead Delivery perso...   \n",
       "2      The facility is clean and level however the st...   \n",
       "3      Absolutely do not bother There is a coffee bar...   \n",
       "4      is the only competent employee here The evenin...   \n",
       "...                                                  ...   \n",
       "49995  Wow Talk about your dichotomy of the absolutel...   \n",
       "49996  Awesome location right on the water Great beac...   \n",
       "49997  We had the and Pork Chop both were really real...   \n",
       "49998  I really the place Its small but intimate Grea...   \n",
       "49999  The quest for the best Sandwich for those of y...   \n",
       "\n",
       "                                               Tokenized  \\\n",
       "0      [[(Tried, VBN), (to, TO), (give, VB), (this, D...   \n",
       "1      [[(My, PRP$), (ordered, JJ), (and, CC), (recei...   \n",
       "2      [[(The, DT), (facility, NN), (is, VBZ), (clean...   \n",
       "3      [[(Absolutely, RB), (do, VBP), (not, RB), (bot...   \n",
       "4      [[(is, VBZ), (the, DT), (only, JJ), (competent...   \n",
       "...                                                  ...   \n",
       "49995  [[(Wow, NNP), (Talk, VBP), (about, IN), (your,...   \n",
       "49996  [[(Awesome, NNP), (location, NN), (right, RB),...   \n",
       "49997  [[(We, PRP), (had, VBD), (the, DT), (and, CC),...   \n",
       "49998  [[(I, PRP), (really, RB), (the, DT), (place, N...   \n",
       "49999  [[(The, DT), (quest, JJS), (for, IN), (the, DT...   \n",
       "\n",
       "       predicted_subjectivity  \n",
       "0                    0.214286  \n",
       "1                    0.688889  \n",
       "2                    0.433333  \n",
       "3                    0.476389  \n",
       "4                    0.594728  \n",
       "...                       ...  \n",
       "49995                0.566667  \n",
       "49996                0.672619  \n",
       "49997                0.550000  \n",
       "49998                0.568519  \n",
       "49999                0.516758  \n",
       "\n",
       "[50000 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1c04fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Original 50k yelp Dataset: 50000\n",
      "Length of UnLabelled Dataset after Textblob Subjectivity: 49639\n",
      "Number of Non Subjective Review removed: 361\n"
     ]
    }
   ],
   "source": [
    "original_length = len(df_yelp)\n",
    "print(\"Length of Original 50k yelp Dataset:\", original_length)\n",
    "\n",
    "# Drop Predicted value = 0.0000 ... i.e. Neutral Review by Textblob Subjectivity\n",
    "df_yelp = df_yelp[df_yelp.predicted_subjectivity != 0]\n",
    "print(\"Length of UnLabelled Dataset after Textblob Subjectivity:\", len(df_yelp))\n",
    "print(\"Number of Non Subjective Review removed:\" , original_length - len(df_yelp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129280b1",
   "metadata": {},
   "source": [
    "### Saving csv file for 50k Yelp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6defe789",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yelp.to_csv('yelp_review_after_subjectivity_classification.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22d1282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
